{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation: Download and transform the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Obtaining dependency information for torchvision from https://files.pythonhosted.org/packages/13/24/23cdf7e7dc33e5c01588c315f8424d31afa9edb05a80168f3d44f7178ff7/torchvision-0.16.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading torchvision-0.16.1-cp311-cp311-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torchvision) (2.1.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision) (2023.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jinja2->torch==2.1.1->torchvision) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sympy->torch==2.1.1->torchvision) (1.3.0)\n",
      "Downloading torchvision-0.16.1-cp311-cp311-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.0/1.1 MB 163.8 kB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.0/1.1 MB 151.3 kB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.0/1.1 MB 151.3 kB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.1/1.1 MB 172.4 kB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.1/1.1 MB 187.3 kB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.1/1.1 MB 187.3 kB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 0.1/1.1 MB 266.9 kB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.1/1.1 MB 266.9 kB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 0.1/1.1 MB 258.0 kB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 0.1/1.1 MB 258.0 kB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.2/1.1 MB 311.3 kB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.2/1.1 MB 335.5 kB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.2/1.1 MB 335.5 kB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 0.3/1.1 MB 387.7 kB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 0.3/1.1 MB 387.7 kB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 0.3/1.1 MB 387.7 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.3/1.1 MB 351.1 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 0.4/1.1 MB 376.1 kB/s eta 0:00:03\n",
      "   --------------- ------------------------ 0.5/1.1 MB 447.5 kB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.5/1.1 MB 447.5 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 0.6/1.1 MB 496.1 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 0.6/1.1 MB 496.1 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 0.6/1.1 MB 493.7 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 0.7/1.1 MB 536.3 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.7/1.1 MB 536.3 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 0.8/1.1 MB 586.0 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 0.8/1.1 MB 586.0 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 0.9/1.1 MB 604.9 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 0.9/1.1 MB 620.2 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 0.9/1.1 MB 620.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 0.9/1.1 MB 596.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 0.9/1.1 MB 596.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 0.9/1.1 MB 596.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 0.9/1.1 MB 596.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 0.9/1.1 MB 596.2 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.1/1.1 MB 592.2 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.1/1.1 MB 592.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 596.3 kB/s eta 0:00:00\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.16.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Download and load the training and test data\n",
    "data_dir = './data'\n",
    "trainset = datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "# Create the DataLoaders\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Set the classes\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Sampels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  bird (tensor(2))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAArWElEQVR4nO3dbWzbZZrv8Z/t2I6TOG7TNk80zWSGMjNQQGcoW9ploDBLRKRFMJ2VmEEaFY2EYHiQqs6I3cILon3RsIxAjNSluzu7YuEsLLxYYJFggK6g7Y66ndP2gOgpc9iylGlmmjQ0beI82Y7t+7xg8ZnQB+6rTXon6fcjWWrsq1fuv/+2L/9j++eIc84JAIAAoqEXAAC4cDGEAADBMIQAAMEwhAAAwTCEAADBMIQAAMEwhAAAwTCEAADBVIVewBeVy2UdOXJE6XRakUgk9HIAAEbOOY2MjKi1tVXR6JmPdWbdEDpy5Ija2tpCLwMAcI56e3u1dOnSM9bM2BB66qmn9LOf/Ux9fX267LLL9OSTT+rb3/72l/6/dDot6bPF19fXz9TygC9VKpW8a63pV8Vi0VZf8q+vqoqbelcnk6Z64Mtks1m1tbVVHs/PZEaG0IsvvqgNGzboqaee0h//8R/rb//2b9XV1aUPPvhAy5YtO+P//fxPcPX19QwhBMUQAs6Nz0sqkZkIMF21apW+9a1vaevWrZXzvvnNb+q2225TT0/PGf9vNptVJpPR8PAwQwhBMYSAs2N5HJ/2d8cVCgXt27dPnZ2dU87v7OzUrl27TqrP5/PKZrNTTgCAC8O0D6Fjx46pVCqpqalpyvlNTU3q7+8/qb6np0eZTKZy4k0JAHDhmLHPCX3xb4HOuVP+fXDTpk0aHh6unHp7e2dqSQCAWWba35iwePFixWKxk456BgYGTjo6kqRkMqkkf5MGgAvStB8JJRIJXXXVVdq2bduU87dt26Y1a9ZM968DAMxhM/IW7Y0bN+qHP/yhVq5cqdWrV+vv/u7vdPjwYd1zzz0z8esAAHPUjAyh22+/XYODg/rLv/xL9fX1acWKFXr99dfV3t4+E78OADBHzcjnhM7F2XxOaJZtwqxgyd2b6evP0t/y2RxJmpyc9K4tFP1rJaloWMvY+Lipd25iwlQ/afhckXV/Ll60yLu2pqbG1DsWi3nXJuIJU+94lX9v8RBxXmWzWS1YsCDM54QAAPDFEAIABMMQAgAEwxACAATDEAIABMMQAgAEwxACAATDEAIABMMQAgAEwxACAAQzI9lx55slogbnnyVGZia/JnvcGJVTMqwlP1kw9c6OjZrqXdl/LTFLnI2kkfEx/95x21eHyxCVFDFE/EhSwvLwxUPEeWV5TOZICAAQDEMIABAMQwgAEAxDCAAQDEMIABAMQwgAEAxDCAAQDEMIABAMQwgAEAxDCAAQDEMIABDMvMiOmz1suWezhXNlU325VDLVRyL+z3Vc2da7ZMgmK5ds+6dkqI86212pVLCtJZFIeNcuXrTY1DtmyGyritmy45LV/uuuitmeEzvL/c141ySP8vzhSAgAEAxDCAAQDEMIABAMQwgAEAxDCAAQDEMIABAMQwgAEAxDCAAQDEMIABAMQwgAEAyxPdNq5qI+TBEln/2HGeOM21ku+ccCFQ21kpTLF/zXUbT1zg6P+Ndm/WslaXTUVl9bW+tdWxXzj8qRpHR9nXftxMSEqXd92n/d1SnbuiPxpHdtIm7rjfOHIyEAQDAMIQBAMAwhAEAwDCEAQDAMIQBAMAwhAEAwDCEAQDAMIQBAMAwhAEAwDCEAQDAMIQBAMPMkO84SlGbLPXPOv7eLzFx2XNkWe6ZCYdK7dnLSv1aSJotFU/3Y6Jh37fDwkKl3f/9R71rDrvxsLUPD3rUDAwOm3vF43FRfVeV/V21qaTH1TtWkvGvj8Zipt1qbvUuPn8ibWi9esMi/dvFiU2/L/V6SIjN435/vOBICAAQz7UOou7tbkUhkyqm52f/ZEADgwjEjf4677LLL9G//9m+Vn2Mx4yE8AOCCMCNDqKqqiqMfAMCXmpHXhA4ePKjW1lZ1dHTo+9//vj7++OPT1ubzeWWz2SknAMCFYdqH0KpVq/Tss8/qzTff1C9+8Qv19/drzZo1GhwcPGV9T0+PMplM5dTW1jbdSwIAzFLTPoS6urr0ve99T5dffrn+5E/+RK+99pok6Zlnnjll/aZNmzQ8PFw59fb2TveSAACz1Ix/Tqi2tlaXX365Dh48eMrLk8mkkkn/74oHAMwfM/45oXw+r9/85jdqMX6ADgAw/037EPrpT3+qHTt26NChQ/r1r3+tP/uzP1M2m9X69eun+1cBAOa4af9z3O9+9zv94Ac/0LFjx7RkyRJdc8012r17t9rb26f7V1U4Z4nMsMVxWKrzkyVT74GBT71rR0ZHTb0t7zLMDvvH00j22J7cRM67diI3Yep94sQJ/+KSLftoYtx/LRMTtnVbY3ssMTKfHjtm6l1Xn/au/epXv2Lq/emn/nFG1dW2P8u7jP91Yo2msu4fnL1pH0IvvPDCdLcEAMxTZMcBAIJhCAEAgmEIAQCCYQgBAIJhCAEAgmEIAQCCYQgBAIJhCAEAgmEIAQCCYQgBAIKZ8a9yOFsjo58qEvXLHEul6r37lsu2uXv8hH+u2ocfHTL1PnjwI+/a030p4OlYsrJc2ZapFpElq0+mr+pIVVebek/m/Lczm7Vl5MlwvZSM12HRmL9nuc6HTgyZeo+PjXvXlozrThty6Vpamky96+v8e8cTtiy4TH3GVB+J2O4T+P84EgIABMMQAgAEwxACAATDEAIABMMQAgAEwxACAATDEAIABMMQAgAEwxACAATDEAIABDNrY3tef/15pWr8Ilza26/w7huPN5jW8Z8HD3vXfvjhQVPv48ePe9daY2Hicf+YkkSV7WZQFXGmektsT94Y21O2ROsUbJEzzvn3tqxDkkqlkqneEgvjSrb9M5n3jz46NuB/m5Wkvv5P/XsPnjD1rk4lvGutqTrVyZSpPmqIVXJl2/6JxmPetSOjo6be8Sr/3pbbeDab9a7lSAgAEAxDCAAQDEMIABAMQwgAEAxDCAAQDEMIABAMQwgAEAxDCAAQDEMIABAMQwgAEAxDCAAQzKzNjjvce0jV1X7ZUP/1cZ9338lCvWkdv/vdsHdtbiJv6i3nnyEVi/lnPElSqsb/+UW5ypZjlq7zz6WTJOf8+1vy2iQpna7zrk0mbesuFAretcWiLZdufGJ8xtYSjdhuK5ZculxuwtR7PJ8zLMSWqfbr3fu8a/dX7zf1vuTiDlO9yv77f3LScJ1Iqk755yn+/ve/N/W2ZExabuMTE/63E46EAADBMIQAAMEwhAAAwTCEAADBMIQAAMEwhAAAwTCEAADBMIQAAMEwhAAAwTCEAADBMIQAAMHM2uy4XK4o5/xm5ET+hHffoRNDpnV8+ql/xlf2xKipd9yQB7dg4QJT71pD3pQlw06SZMwmq6lJetcubbvI1PvSSy/1rh08dtzUe2LCP+NrfNyWBWfN+Mrn/XMJS0Vb/l5hctK7dmxszNS7WPZfSz7vn48nSQNHB71rYxFbPmJ2eMBUvzDjfxtPJv2z+iQpYsrUs92XJyf9653hcSJvyAzkSAgAEIx5CO3cuVO33HKLWltbFYlE9Morr0y53Dmn7u5utba2KpVKae3atTpw4MB0rRcAMI+Yh9DY2JiuvPJKbdmy5ZSXP/bYY3riiSe0ZcsW7dmzR83Nzbrppps0MjJyzosFAMwv5teEurq61NXVdcrLnHN68skn9fDDD2vdunWSpGeeeUZNTU16/vnndffdd5/bagEA88q0viZ06NAh9ff3q7Ozs3JeMpnU9ddfr127dp3y/+TzeWWz2SknAMCFYVqHUH9/vySpqalpyvlNTU2Vy76op6dHmUymcmpra5vOJQEAZrEZeXfcF78u2Dl32q8Q3rRpk4aHhyun3t7emVgSAGAWmtbPCTU3N0v67IiopaWlcv7AwMBJR0efSyaTSib932MPAJg/pvVIqKOjQ83Nzdq2bVvlvEKhoB07dmjNmjXT+asAAPOA+UhodHRUH330UeXnQ4cO6b333lNDQ4OWLVumDRs2aPPmzVq+fLmWL1+uzZs3q6amRnfccce0LhwAMPeZh9DevXt1ww03VH7euHGjJGn9+vX6x3/8Rz344IOamJjQvffeqxMnTmjVqlV66623lE6nTb/notZWpVJ+f6YbHfePEolGbPEqA0eHvGtrUrY4m8WLFnvXjho/ZxWRfwRKqjpl6l0Vt21nW7v/m006OjpMvaNR/wiUhoaFpt6pVK13bS7nH1MiSY2NS0z1lmSld//3u6beg8f942/yef+IH0nKTfrfDo8P2W7jsdO8znwq9XW2P/lXV9eb6qOxGu/aXM72GDQyMuRdWyoXTb0nC/77xxbb49/XPITWrl17xsVEIhF1d3eru7vb2hoAcIEhOw4AEAxDCAAQDEMIABAMQwgAEAxDCAAQDEMIABAMQwgAEAxDCAAQDEMIABAMQwgAEMy0fpXDdFqwwKmmxi+rqCpR7d03GvXPA5OksWzZu7a2OmPqXRzPe9d++P57pt6LF/qvJbO00dQ7lbZlzWUWLvCuLcn/+pak0Zx/bmBVxHZzTxnW4pxt3VVx413PkB23ZEmDqfXhw7/1rh0aGjb1Hh71z9QbG/fPG5MklUvepYmE7fn2yIgt268mdeqvqjmVZcu+auq9eIn/zjfE6UmSyobbbdRwFY6Pj0v6n359/dsCADC9GEIAgGAYQgCAYBhCAIBgGEIAgGAYQgCAYBhCAIBgGEIAgGAYQgCAYBhCAIBgZm1sz8DRnKpTfnEVZSW9+1bFbLE9S5oW+Rfn/SNkJCk7csK7dnF93NQ7U+u/a9ta/CNHJKmuYYGpPh6LedeWCpOm3jFD79pM2tQ7kfS/DgvGxBnJP3JGknJ5//ibzAJbfFRtnf99onTkqKl3zPlvp5v0j7GSpFFDzE/ecP1J0nB23FRfu8A/KmnZxctNvb956cXetZnMAlPvWNTyuOKfCTQykvWu5UgIABAMQwgAEAxDCAAQDEMIABAMQwgAEAxDCAAQDEMIABAMQwgAEAxDCAAQDEMIABAMQwgAEMyszY6rSmYUT1Z71VYn/XObSs6WwVYcGPGvzRVNvRcuXOxd+z++Zcu8a156kXdtXbrG1Dvi/DL9Pnfww//0rq2vt+W7XX311d61kah/9pUkxRP+uXSxKlvvSMR2HQ4NHfeunZiw3Q7r6/2z5hYuqDf1Hv6vT7xryzlb9mJd3O/xQZJixtvsxLj/9S1Jk8VR79qxiWOm3kMn/LMxx8f6TL1TqZT/Oob8sy7Hxvz3JUdCAIBgGEIAgGAYQgCAYBhCAIBgGEIAgGAYQgCAYBhCAIBgGEIAgGAYQgCAYBhCAIBgZm1sT8kVVHJ+USh9Rz/17js0XDCt49gx/6iKqpJ/zIskDU1OeNcuarDFpTS2LfOuramrM/XOTeRN9WMjWe/ai7/6VVPv2pR/5NDImH8EkyTlcuPetZGoLRamXJ401e/b97+8azOZJabeyYT/dRiP2eKJBo4c9q5NVNmeE3/lK23etbmS7aEuYbxP1KZy3rXZ4x+beh+WfxRPqey/DkmKJ8retZNF/yieCcNjBEdCAIBgGEIAgGDMQ2jnzp265ZZb1NraqkgkoldeeWXK5XfeeacikciU0zXXXDNd6wUAzCPmITQ2NqYrr7xSW7ZsOW3NzTffrL6+vsrp9ddfP6dFAgDmJ/MbE7q6utTV1XXGmmQyqebm5rNeFADgwjAjrwlt375djY2NuuSSS3TXXXdpYGDgtLX5fF7ZbHbKCQBwYZj2IdTV1aXnnntOb7/9th5//HHt2bNHN954o/L5U79lr6enR5lMpnJqa/N/2yUAYG6b9s8J3X777ZV/r1ixQitXrlR7e7tee+01rVu37qT6TZs2aePGjZWfs9ksgwgALhAz/mHVlpYWtbe36+DBg6e8PJlMKpn0/w51AMD8MeOfExocHFRvb69aWlpm+lcBAOYY85HQ6OioPvroo8rPhw4d0nvvvaeGhgY1NDSou7tb3/ve99TS0qJPPvlEDz30kBYvXqzvfve707pwAMDcZx5Ce/fu1Q033FD5+fPXc9avX6+tW7dq//79evbZZzU0NKSWlhbdcMMNevHFF5VOp02/JxIpKRIpedUOnRj07vvpUf88MEmanPTPyoqmbPluC5sbvWuXX3qJqXdDq/+RZyqVMPVeWltrqv/GZZd519bU2DK7hob930159OgxU285/1ytpkZbXltDZpGpvj7lf/8Zyw6beqeb/K/zjCGrT5Lamxd616ZStly6unr/P+Onjbfx5Sv8sxclqT7tf73UJGz5bpOlUe/aqirbdRiN+uddxmL+94dY1L/WPITWrl0r504f1vjmm29aWwIALlBkxwEAgmEIAQCCYQgBAIJhCAEAgmEIAQCCYQgBAIJhCAEAgmEIAQCCYQgBAIJhCAEAgpnxr3I4W1VVcVVVeeY9nSFG6IsmC0XbOmL+uVoLG2zZccval/oX+0c8SZI+/M//6127uMGWY9bSZEtEd84/z6pUOm7qXSz6Z1QNHLNlquVO80WMp5KfNLVWwwJbluK113/Hu/b40Om/yfhU8rmCd+3YyIipd2aJ/22lNlVt6p33v9ursdl637zIVF9T7X87dJO27Liy8z9WsH4tTiTif9/M5fzXMRk3ZG56VwIAMM0YQgCAYBhCAIBgGEIAgGAYQgCAYBhCAIBgGEIAgGAYQgCAYBhCAIBgGEIAgGBmbWxPuVxUueyXhVIs+kfxRKO2TU7E/WMw8uPjpt4jwye8a3PjWVPvgYE+/9q+35t6H+3rN9V7xy9JisXipt6lon92y0TBlq1zIut/nR/tt10niSrb87/aGv9Im4QtuUXFkn9sT7Tatu7RSf84myPHbJFNNfX+kVo1I7Z9PzFsuy/Hyv6PQZFIydS7XPa/jcditnwvy2NnLucfN5TL+UdecSQEAAiGIQQACIYhBAAIhiEEAAiGIQQACIYhBAAIhiEEAAiGIQQACIYhBAAIhiEEAAiGIQQACGbWZseNTwyqLM/MsYh/9lU+b8uEmsz75zbVpZeYesfk3zs/MWbq3bRksXftuLH3b3sPmuqdf3yYnLM9LyqV/K/DgYFBU++yIZcuXVdj6p2M2zK+XMk/b2yy6J/bJUmFSf/7RFOz7Tb+9W8s8679PwcOmXrnnX/u2eio7TY+2HfMVJ+M1XvXJmqsD7v+uXeFgv9joSRNTvr3ttUa8jy9KwEAmGYMIQBAMAwhAEAwDCEAQDAMIQBAMAwhAEAwDCEAQDAMIQBAMAwhAEAwDCEAQDCzNranJhlVKuk3I2tT/psRjfpHT0jS8NCn3rV16WpT70TCf90jIydMvYezx71rJ4u268RqbHTUuzY3botXiUYi3rW/6+0z9U7GU961CxdkTL3jMVtsT1Xc/7ZiqZWkciTnXRuJ2iKBEkn/OKO6dNLUe3DIf93j4xOm3hP5tKl+bNw/VilaZds/8YT/sUK5bMjIklQVj/sXx/zXUXL+90uOhAAAwZiGUE9Pj66++mql02k1Njbqtttu04cffjilxjmn7u5utba2KpVKae3atTpw4MC0LhoAMD+YhtCOHTt03333affu3dq2bZuKxaI6Ozs1Nvb//4Ty2GOP6YknntCWLVu0Z88eNTc366abbtLIyMi0Lx4AMLeZ/jj5xhtvTPn56aefVmNjo/bt26frrrtOzjk9+eSTevjhh7Vu3TpJ0jPPPKOmpiY9//zzuvvuu6dv5QCAOe+cXhMaHh6WJDU0NEiSDh06pP7+fnV2dlZqksmkrr/+eu3ateuUPfL5vLLZ7JQTAODCcNZDyDmnjRs36tprr9WKFSskSf39/ZKkpqamKbVNTU2Vy76op6dHmUymcmprazvbJQEA5pizHkL333+/3n//ff3zP//zSZdFvvC2WefcSed9btOmTRoeHq6cent7z3ZJAIA55qw+J/TAAw/o1Vdf1c6dO7V06dLK+c3NzZI+OyJqaWmpnD8wMHDS0dHnksmkkknb5wMAAPOD6UjIOaf7779fL730kt5++211dHRMubyjo0PNzc3atm1b5bxCoaAdO3ZozZo107NiAMC8YToSuu+++/T888/rX//1X5VOpyuv82QyGaVSKUUiEW3YsEGbN2/W8uXLtXz5cm3evFk1NTW64447ZmQDAABzl2kIbd26VZK0du3aKec//fTTuvPOOyVJDz74oCYmJnTvvffqxIkTWrVqld566y2l07YYDADA/GcaQs65L62JRCLq7u5Wd3f32a5JkpSKx5TyzMCqq/Xvu2SJbRg6N+5dO/DpgKn34HH/XLpczn8dknS495B3rSvZ8qZizvZ+lnLJP5suHvXPnLL2Lhjzw8pxQ++cLfPO+jpoXV2dd+2ixgZT70xmkX9x1PYy8tBwwbs2l7Pt+0TUP/csIv9sN0lyMdt9oro24d87UjT1PjHonwNpiGyTJEUN+ZWRuH/e4cSk/34nOw4AEAxDCAAQDEMIABAMQwgAEAxDCAAQDEMIABAMQwgAEAxDCAAQDEMIABAMQwgAEMxZfZXD+RCJlhWJ+kVtVNf4R3I0tWRM66itW+hdO3DU9q2wExN5/3XUVpt6p1Ip79pCzj9iQ5JGTwyZ6ifGR71ry0X/qBxJmsj710eMz7nyef/9Uy7bYl5GRkZM9YODx7xrjxw9Yurd0nqRd21jk+1LJweP+8dNHTvmH08jSfUL/KOPGlsWm3rHE7aYn8mS/23cJ/5sSr3862Mx/2gdSYpV+Y+AXMH/caJY8L9fciQEAAiGIQQACIYhBAAIhiEEAAiGIQQACIYhBAAIhiEEAAiGIQQACIYhBAAIhiEEAAiGIQQACGbWZscdGzyi6uq4V62L+mdIJar9s+AkKRPzz5qrrq4z9R4bzXnXFgpFU+9MvX+W2YQxOy5R7Z9LJ0njo/45aaPZIVvvgn++W03C7/Z0NkolY9bYpC0jz1I/mvXPa5Ok3sl+/96jttyz8bz/WkqaMPVe1NTiXbu0PW3qXZe2ZbCVS/73z2I5YuqdMtzfnK21LImHlsw7Sy1HQgCAYBhCAIBgGEIAgGAYQgCAYBhCAIBgGEIAgGAYQgCAYBhCAIBgGEIAgGAYQgCAYGZtbE88EVc86RezUpZ/RMRk3haXEov452Ck07Wm3tVJ/ziO48f9o28kKZ8b865NGtYhSZmGRab6mjr/OKP6BbZYpWii2rt2YmTI1FuGKBZrDE80ZouFiScT3rUuaus9WfJf+/BIn6l3qs5/LV/76mJT7698td67Nr3A1Frxatvz83jS/3ZojmwyhOtY46OKk/71pfLMrIMjIQBAMAwhAEAwDCEAQDAMIQBAMAwhAEAwDCEAQDAMIQBAMAwhAEAwDCEAQDAMIQBAMAwhAEAwszY7LpGqVjLll5eVyxW8+1ZFbZscr/LPVUtU15h6F+L+WUwTE/45ZpJUKlqeX/jn40lSIm67DvMxvwxASSrG/TO4JCkW9e99/Jh//pokjQwd964tOf99KUmK2a7z+lr/29aiZuP+rPavr62zPW/NLEx61za12HID6zL++77Kv1SSFI0b8/ecfx5cOeqfdSlJRed/3y+Xbdlxzvmvpcpwv68q+t8fOBICAARjGkI9PT26+uqrlU6n1djYqNtuu00ffvjhlJo777xTkUhkyumaa66Z1kUDAOYH0xDasWOH7rvvPu3evVvbtm1TsVhUZ2enxsamfm3AzTffrL6+vsrp9ddfn9ZFAwDmB9Mf9994440pPz/99NNqbGzUvn37dN1111XOTyaTam5unp4VAgDmrXN6TWh4eFiS1NDQMOX87du3q7GxUZdcconuuusuDQwMnLZHPp9XNpudcgIAXBjOegg557Rx40Zde+21WrFiReX8rq4uPffcc3r77bf1+OOPa8+ePbrxxhuVz+dP2aenp0eZTKZyamtrO9slAQDmmLN+i/b999+v999/X7/61a+mnH/77bdX/r1ixQqtXLlS7e3teu2117Ru3bqT+mzatEkbN26s/JzNZhlEAHCBOKsh9MADD+jVV1/Vzp07tXTp0jPWtrS0qL29XQcPHjzl5clkUsmk/2cJAADzh2kIOef0wAMP6OWXX9b27dvV0dHxpf9ncHBQvb29amlpOetFAgDmJ9NrQvfdd5/+6Z/+Sc8//7zS6bT6+/vV39+viYkJSdLo6Kh++tOf6j/+4z/0ySefaPv27brlllu0ePFiffe7352RDQAAzF2mI6GtW7dKktauXTvl/Kefflp33nmnYrGY9u/fr2effVZDQ0NqaWnRDTfcoBdffFHpdHraFg0AmB/Mf447k1QqpTfffPOcFvS5idy4XMQvj6lc8g+GcmVbrla6zj+zK5WyDdr8Cf+3o8eNeW01hhi7fN4/e0+SSiXbdVgu++dIjY+Pm3rHYv4ZX/WZBabehfzYlxf9NxezZcdVGV+NbViU8a5d0my7HdYY8uBSNbY31Kbr/fP6Eklb70LRP69tctK/VpJKpWFTfTxu2M6ELcOwUMh51xaLtozJaNT/Oo8a4vTKhrw7suMAAMEwhAAAwTCEAADBMIQAAMEwhAAAwTCEAADBMIQAAMEwhAAAwTCEAADBMIQAAMGc9fcJzTQXScpF/OJ48gX/PIlIyRaZURWv9a4tFm1xNrkJ/7ic030p4OlYIk3yBWPvfMlUb2GJEZG+PErqDyWS1abeixqb/NchW/RRbZ1/1JQklZx/dEv9AmPEU63/7Xay5L8OSXKGu0TZGXJhJEUNOTLW21W5bIu/KZX9r5cxYzRVbsL//hmJ2LazVPLfzlLJPwssl/N//OFICAAQDEMIABAMQwgAEAxDCAAQDEMIABAMQwgAEAxDCAAQDEMIABAMQwgAEAxDCAAQDEMIABDMrM2OmyxGFZv0y4Zy5TrvvnW1LaZ1xGJp79rscNbUu1DwzxsbHR0x9Zb8M9UmJmx5YCNZW/ZVJOIfIJZMJk29LddhxJgftqChwbu2Km7L0yvLdh2WnX//mHEtEUNk22R+wtT7+PFh79p0vf99TZKc4Tl0ImHLjHTOlh03ach2tN7Gqwwxg86VTb1zhnWf6DvmXVvI+19/HAkBAIJhCAEAgmEIAQCCYQgBAIJhCAEAgmEIAQCCYQgBAIJhCAEAgmEIAQCCYQgBAIKZtbE9mdplSqX84i0SCxZ5912YWWZaR7nkfxXlJmyRJtXV1d61mcwCU2+LqqoxU30sZnvuEo3658JkMhlT71LJPx4kXmWLS0lWG+oj/vEnklQs2a7zeNI/hiltuwqlqH/00RLZYmGKJf/rJZE05NNIKhYnDbW2GJ5y2badEflHX8Uitu1M1Rgih5whg0nSgnTKu7awwP86yeUKkvZ51XIkBAAIhiEEAAiGIQQACIYhBAAIhiEEAAiGIQQACIYhBAAIhiEEAAiGIQQACIYhBAAIhiEEAAgm4pzzD6U6D7LZrDKZjH5/5L9UX5/2+j+lYsS7fyzmn9cmSZYIqWLRP4Prs962fCoLy261rqNUKpnqo1H/5zqRiP++tNZbe1vW7YyZas54nUejltu4qbVKZf8MNifbw4XhKjTvH8m/3pVt6y472/4plwz1EdtaLPvTOdtxRcQQHxo1ZN5lsyP6yle+puHhYdXX15+5r3dXAACmmWkIbd26VVdccYXq6+tVX1+v1atX65e//GXlcuecuru71draqlQqpbVr1+rAgQPTvmgAwPxgGkJLly7Vo48+qr1792rv3r268cYbdeutt1YGzWOPPaYnnnhCW7Zs0Z49e9Tc3KybbrpJIyMjM7J4AMDcds6vCTU0NOhnP/uZfvSjH6m1tVUbNmzQn//5n0uS8vm8mpqa9Fd/9Ve6++67vfrxmtD04DWhc+/Na0In4zWh09TzmtAU5+U1oVKppBdeeEFjY2NavXq1Dh06pP7+fnV2dlZqksmkrr/+eu3ateu0ffL5vLLZ7JQTAODCYB5C+/fvV11dnZLJpO655x69/PLLuvTSS9Xf3y9JampqmlLf1NRUuexUenp6lMlkKqe2tjbrkgAAc5R5CH3961/Xe++9p927d+vHP/6x1q9frw8++KBy+RcPqZ1zZzzM3rRpk4aHhyun3t5e65IAAHOU/x8E/1sikdDFF18sSVq5cqX27Nmjn//855XXgfr7+9XS0lKpHxgYOOno6A8lk0klk0nrMgAA88A5f07IOad8Pq+Ojg41Nzdr27ZtlcsKhYJ27NihNWvWnOuvAQDMQ6YjoYceekhdXV1qa2vTyMiIXnjhBW3fvl1vvPGGIpGINmzYoM2bN2v58uVavny5Nm/erJqaGt1xxx0ztX4AwBxmGkJHjx7VD3/4Q/X19SmTyeiKK67QG2+8oZtuukmS9OCDD2piYkL33nuvTpw4oVWrVumtt95SOu33Vus/VFe7WHW1Z35rH4DpYHnLsPVt1Dh3lreLz+T+sXxMwP8lllmbHefz/nIA04EhNLvNvSFkeRwnOw4AEAxDCAAQDEMIABAMQwgAEAxDCAAQDEMIABAMQwgAEAxDCAAQDEMIABCMOUV7pn0e4MCX2wHnC4kJs9vcTEyQ/L7hedYNoZGREUniy+0AYI4bGRlRJpM5Y82sy44rl8s6cuSI0un0lC/Dy2azamtrU29v77zOlGM7548LYRsltnO+mY7tdM5pZGREra2tikbP/KrPrDsSikajWrp06Wkvr6+vn9c3gM+xnfPHhbCNEts535zrdn7ZEdDneGMCACAYhhAAIJg5M4SSyaQeeeQRJZP+X5Y0F7Gd88eFsI0S2znfnO/tnHVvTAAAXDjmzJEQAGD+YQgBAIJhCAEAgmEIAQCCmTND6KmnnlJHR4eqq6t11VVX6d///d9DL2ladXd3KxKJTDk1NzeHXtY52blzp2655Ra1trYqEonolVdemXK5c07d3d1qbW1VKpXS2rVrdeDAgTCLPQdftp133nnnSfv2mmuuCbPYs9TT06Orr75a6XRajY2Nuu222/Thhx9OqZkP+9NnO+fD/ty6dauuuOKKygdSV69erV/+8peVy8/nvpwTQ+jFF1/Uhg0b9PDDD+vdd9/Vt7/9bXV1denw4cOhlzatLrvsMvX19VVO+/fvD72kczI2NqYrr7xSW7ZsOeXljz32mJ544glt2bJFe/bsUXNzs2666aZKfuBc8WXbKUk333zzlH37+uuvn8cVnrsdO3bovvvu0+7du7Vt2zYVi0V1dnZqbGysUjMf9qfPdkpzf38uXbpUjz76qPbu3au9e/fqxhtv1K233loZNOd1X7o54I/+6I/cPffcM+W8b3zjG+4v/uIvAq1o+j3yyCPuyiuvDL2MGSPJvfzyy5Wfy+Wya25udo8++mjlvFwu5zKZjPubv/mbACucHl/cTuecW79+vbv11luDrGemDAwMOElux44dzrn5uz+/uJ3Ozc/96ZxzCxcudH//939/3vflrD8SKhQK2rdvnzo7O6ec39nZqV27dgVa1cw4ePCgWltb1dHRoe9///v6+OOPQy9pxhw6dEj9/f1T9msymdT1118/7/arJG3fvl2NjY265JJLdNddd2lgYCD0ks7J8PCwJKmhoUHS/N2fX9zOz82n/VkqlfTCCy9obGxMq1evPu/7ctYPoWPHjqlUKqmpqWnK+U1NTerv7w+0qum3atUqPfvss3rzzTf1i1/8Qv39/VqzZo0GBwdDL21GfL7v5vt+laSuri4999xzevvtt/X4449rz549uvHGG5XP50Mv7aw457Rx40Zde+21WrFihaT5uT9PtZ3S/Nmf+/fvV11dnZLJpO655x69/PLLuvTSS8/7vpx1Kdqn84df6yB9dgP54nlzWVdXV+Xfl19+uVavXq2vfe1reuaZZ7Rx48aAK5tZ832/StLtt99e+feKFSu0cuVKtbe367XXXtO6desCruzs3H///Xr//ff1q1/96qTL5tP+PN12zpf9+fWvf13vvfeehoaG9C//8i9av369duzYUbn8fO3LWX8ktHjxYsVisZMm8MDAwEmTej6pra3V5ZdfroMHD4Zeyoz4/J1/F9p+laSWlha1t7fPyX37wAMP6NVXX9U777wz5StX5tv+PN12nspc3Z+JREIXX3yxVq5cqZ6eHl155ZX6+c9/ft735awfQolEQldddZW2bds25fxt27ZpzZo1gVY18/L5vH7zm9+opaUl9FJmREdHh5qbm6fs10KhoB07dszr/SpJg4OD6u3tnVP71jmn+++/Xy+99JLefvttdXR0TLl8vuzPL9vOU5mL+/NUnHPK5/Pnf19O+1sdZsALL7zg4vG4+4d/+Af3wQcfuA0bNrja2lr3ySefhF7atPnJT37itm/f7j7++GO3e/du96d/+qcunU7P6W0cGRlx7777rnv33XedJPfEE0+4d9991/32t791zjn36KOPukwm41566SW3f/9+94Mf/MC1tLS4bDYbeOU2Z9rOkZER95Of/MTt2rXLHTp0yL3zzjtu9erV7qKLLppT2/njH//YZTIZt337dtfX11c5jY+PV2rmw/78su2cL/tz06ZNbufOne7QoUPu/fffdw899JCLRqPurbfecs6d3305J4aQc8799V//tWtvb3eJRMJ961vfmvKWyfng9ttvdy0tLS4ej7vW1la3bt06d+DAgdDLOifvvPOOk3TSaf369c65z97W+8gjj7jm5maXTCbddddd5/bv3x920WfhTNs5Pj7uOjs73ZIlS1w8HnfLli1z69evd4cPHw69bJNTbZ8k9/TTT1dq5sP+/LLtnC/780c/+lHl8XTJkiXuO9/5TmUAOXd+9yVf5QAACGbWvyYEAJi/GEIAgGAYQgCAYBhCAIBgGEIAgGAYQgCAYBhCAIBgGEIAgGAYQgCAYBhCAIBgGEIAgGAYQgCAYP4fI9/taPNHfHwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the default figure background color to white\n",
    "plt.rcParams['figure.facecolor'] = '#ffffff'\n",
    "\n",
    "# Function to un-normalize and display an image\n",
    "def imshow(img):\n",
    "    # Unnormalize the image\n",
    "    img = img / 2 + 0.5\n",
    "    # Convert from Tensor image\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    # Show the image\n",
    "    plt.show()\n",
    "\n",
    "# Function to show an example with the corresponding label\n",
    "def show_example(img, label):\n",
    "    print('Label: ', classes[label], \"(\" + str(label) + \")\")\n",
    "    imshow(img)\n",
    "\n",
    "# Get some random training images with their labels\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Choose a random image to show\n",
    "img = images[0]\n",
    "label = labels[0]\n",
    "\n",
    "# Call the show_example function\n",
    "show_example(img, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogLeNet \n",
    "\n",
    "### Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out1x1, red3x3, out3x3, red5x5, out5x5, out1x1pool):\n",
    "        super(InceptionModule, self).__init__()\n",
    "\n",
    "        self.branch1 = nn.Conv2d(in_channels, out1x1, kernel_size=1)\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, red3x3, kernel_size=1),\n",
    "            nn.Conv2d(red3x3, out3x3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, red5x5, kernel_size=1),\n",
    "            nn.Conv2d(red5x5, out5x5, kernel_size=5, padding=2)\n",
    "        )\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, out1x1pool, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "\n",
    "        return torch.cat([branch1, branch2, branch3, branch4], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogLeNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (inception3a): InceptionModule(\n",
      "    (branch1): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (branch2): Sequential(\n",
      "      (0): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (branch3): Sequential(\n",
      "      (0): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    )\n",
      "    (branch4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (inception3b): InceptionModule(\n",
      "    (branch1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (branch2): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (branch3): Sequential(\n",
      "      (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(32, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    )\n",
      "    (branch4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (maxpool3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (inception4a): InceptionModule(\n",
      "    (branch1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (branch2): Sequential(\n",
      "      (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (branch3): Sequential(\n",
      "      (0): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(16, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    )\n",
      "    (branch4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (inception4b): InceptionModule(\n",
      "    (branch1): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (branch2): Sequential(\n",
      "      (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (branch3): Sequential(\n",
      "      (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    )\n",
      "    (branch4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (inception4c): InceptionModule(\n",
      "    (branch1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (branch2): Sequential(\n",
      "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (branch3): Sequential(\n",
      "      (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    )\n",
      "    (branch4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (inception4d): InceptionModule(\n",
      "    (branch1): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (branch2): Sequential(\n",
      "      (0): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (branch3): Sequential(\n",
      "      (0): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    )\n",
      "    (branch4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (inception4e): InceptionModule(\n",
      "    (branch1): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (branch2): Sequential(\n",
      "      (0): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (branch3): Sequential(\n",
      "      (0): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    )\n",
      "    (branch4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (maxpool4): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (inception5a): InceptionModule(\n",
      "    (branch1): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (branch2): Sequential(\n",
      "      (0): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (branch3): Sequential(\n",
      "      (0): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    )\n",
      "    (branch4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (inception5b): InceptionModule(\n",
      "    (branch1): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (branch2): Sequential(\n",
      "      (0): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (branch3): Sequential(\n",
      "      (0): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(48, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    )\n",
      "    (branch4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.inception3a = InceptionModule(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3b = InceptionModule(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.inception4a = InceptionModule(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b = InceptionModule(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c = InceptionModule(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = InceptionModule(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4e = InceptionModule(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.inception5a = InceptionModule(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b = InceptionModule(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        x = self.inception4a(x)\n",
    "        x = self.inception4b(x)\n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "        x = self.inception4e(x)\n",
    "        x = self.maxpool4(x)\n",
    "\n",
    "        x = self.inception5a(x)\n",
    "        x = self.inception5b(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Create an instance of the GoogLeNet model\n",
    "googlenet = GoogLeNet()\n",
    "\n",
    "# Print the model architecture\n",
    "print(googlenet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 3.023\n",
      "[1,   200] loss: 2.284\n",
      "[1,   300] loss: 2.088\n",
      "[1,   400] loss: 2.016\n",
      "[1,   500] loss: 2.005\n",
      "[1,   600] loss: 1.941\n",
      "[1,   700] loss: 1.882\n",
      "[1,   800] loss: 1.958\n",
      "[1,   900] loss: 1.851\n",
      "[1,  1000] loss: 1.830\n",
      "[1,  1100] loss: 1.798\n",
      "[1,  1200] loss: 1.721\n",
      "[1,  1300] loss: 1.694\n",
      "[1,  1400] loss: 1.640\n",
      "[1,  1500] loss: 1.667\n",
      "[2,   100] loss: 1.527\n",
      "[2,   200] loss: 1.535\n",
      "[2,   300] loss: 1.494\n",
      "[2,   400] loss: 1.460\n",
      "[2,   500] loss: 1.446\n",
      "[2,   600] loss: 1.436\n",
      "[2,   700] loss: 1.473\n",
      "[2,   800] loss: 1.467\n",
      "[2,   900] loss: 1.386\n",
      "[2,  1000] loss: 1.366\n",
      "[2,  1100] loss: 1.318\n",
      "[2,  1200] loss: 1.321\n",
      "[2,  1300] loss: 1.335\n",
      "[2,  1400] loss: 1.288\n",
      "[2,  1500] loss: 1.314\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(googlenet.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = googlenet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test images: 53 %\n"
     ]
    }
   ],
   "source": [
    "# Test the trained model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = googlenet(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on the test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Basic block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * out_channels)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = torch.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        # Initial convolutional layer\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Residual blocks\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "        # Global average pooling\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Function to create ResNet-18\n",
    "def resnet18(num_classes=1000):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "# Create an instance of the ResNet-18 model\n",
    "resnet = resnet18()\n",
    "\n",
    "# Print the model architecture\n",
    "print(resnet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.065\n",
      "[1,   200] loss: 1.721\n",
      "[1,   300] loss: 1.606\n",
      "[1,   400] loss: 1.576\n",
      "[1,   500] loss: 1.525\n",
      "[1,   600] loss: 1.434\n",
      "[1,   700] loss: 1.387\n",
      "[1,   800] loss: 1.336\n",
      "[1,   900] loss: 1.322\n",
      "[1,  1000] loss: 1.266\n",
      "[1,  1100] loss: 1.242\n",
      "[1,  1200] loss: 1.217\n",
      "[1,  1300] loss: 1.201\n",
      "[1,  1400] loss: 1.157\n",
      "[1,  1500] loss: 1.127\n",
      "[2,   100] loss: 1.076\n",
      "[2,   200] loss: 1.030\n",
      "[2,   300] loss: 0.984\n",
      "[2,   400] loss: 0.971\n",
      "[2,   500] loss: 1.016\n",
      "[2,   600] loss: 0.958\n",
      "[2,   700] loss: 0.968\n",
      "[2,   800] loss: 0.961\n",
      "[2,   900] loss: 0.900\n",
      "[2,  1000] loss: 0.914\n",
      "[2,  1100] loss: 0.817\n",
      "[2,  1200] loss: 0.866\n",
      "[2,  1300] loss: 0.849\n",
      "[2,  1400] loss: 0.865\n",
      "[2,  1500] loss: 0.852\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model for 2 epochs\n",
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test images: 70 %\n"
     ]
    }
   ],
   "source": [
    "# Test the trained model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = resnet(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on the test images: %d %%' % (100 * correct / total))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
